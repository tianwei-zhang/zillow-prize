
---
title: "0925 Data Review Zillow Competition"
output: html_notebook
---

```{r}
rm(list=ls())
library(tianweiR)
library(dplyr)
library(ggplot2)
library(data.table)
library(caret)
```

# load data
```{r}
data_property=fread('Data/properties_2016.csv',na.strings = c('NA','na','',' '))
data_train=fread('Data/train_2016_v2.csv',na.strings = c('NA','na','',' '))
data_test=fread('Data/sample_submission.csv',header = T,na.strings = c('NA','na','',' '))
```

## link property to error file
```{r}
data_all=data_train%>%left_join(data_property)
```

```{r}
head(data_all)
```


```{r}
str(data_all)
```
## give NA 0 values
```{r}
data_all[is.na(data_all)]=0
```

## Correct data type
```{r}
factor_col_index=which(colnames(data_all) %like% 'id')
colnames(data_all)[factor_col_index]
for (i in factor_col_index){
  data_all[,i]=as.factor(data_all[,i])
}
```

Correct columns that are factors with no id in names

roomcnt missing?? why, probably cannot give it 0
```{r}
data_all$hashottuborspa=as.factor(data_all$hashottuborspa)
data_all$propertycountylandusecode=as.factor(data_all$propertycountylandusecode)
data_all$rawcensustractandblock=as.factor(data_all$rawcensustractandblock)
data_all$taxdelinquencyflag=as.factor(data_all$taxdelinquencyflag)
```

# Explorary Data Analysis

we have some records with multiple entries
```{r}
data_train%>%
  group_by(parcelid)%>%
  summarise(num_entry=length(unique(transactiondate)))%>%
  filter(num_entry>1)%>%
  arrange(-num_entry)
```

```{r}
data_1=data_train%>%filter(parcelid=='10821829')
```

### Check distribution of all numeric columns
```{r,eval=FALSE}
  num_col_index=seq(1:ncol(data_all))[sapply(data_all,is.numeric)]

  for(i in num_col_index){
    hist(data_all[,i],50,main = paste('Distribution of',colnames(data_all)[i]),xlab=colnames(data_all)[i])
  }
  
```

### continue to correct variables 
```{r}
summary(data_all$basementsqft)
```

looks like only 1 value existed
```{r}
data_all=data_all%>%
  select(
    -basementsqft,
    -finishedfloor1squarefeet,
    -finishedsquarefeet13,
    -finishedsquarefeet15,
    -finishedsquarefeet50,
    -finishedsquarefeet6,
    -lotsizesquarefeet,
    -poolsizesum,
    -unitcnt,
    -yardbuildingsqft17,
    -yardbuildingsqft26
    
  )
```

```{r}
str(data_all)
```

## correlation analysis
```{r}
plot(data_all$logerror,data_all$taxvaluedollarcnt)
```

### Cap outliers
```{r}
num_col_index2=seq(1,ncol(data_all))[sapply(data_all,is.numeric)]
data_all=cap_outlier_iqr(data_all,num_col_index2[-1])
```



# Prepare training data
```{r}
# library(sqldf)
# data_unique=sqldf('select a.*
# from data_all a
# inner join (select parcelid, min(date(transactiondate)) min_tran from data_all group by parcelid) b
# on a.parcelid=b.parcelid and a.transactiondate=b.min_tran
#       ',method = "raw")
# 
# nrow(data_unique)


unique_records=data_all%>%group_by(parcelid)%>%summarise(min_tran=as.character(min(as.Date(transactiondate))))%>%ungroup()
data_unique=data_all%>%inner_join(unique_records,by=c('parcelid','transactiondate'='min_tran'))
nrow(data_unique)
```

```{r}
length(unique(data_all$parcelid))
```

### Remove factors withi many levels
```{r}
summary(data_unique)
```


### remove additional factors with too many levels
```{r}
data_unique=data_unique%>%
  select(-censustractandblock,
         -taxdelinquencyyear,
         -regionidzip,
         -regionidneighborhood,
         -regionidcity,
         -rawcensustractandblock,
         -propertycountylandusecode,
         -parcelid)
```

```{r}
data_unique$assessmentyear=as.factor(data_unique$assessmentyear)
data_unique$fireplaceflag=as.factor(data_unique$fireplaceflag)
data_unique$yearbuilt=as.factor(data_unique$yearbuilt)
data_unique$yearbuilt=as.factor(data_unique$yearbuilt)


```

```{r}
summary(data_unique$roomcnt)
data_unique=data_unique%>%select(-roomcnt)
```



### normalize numeric variables
```{r}
num_col_index3=seq(1,ncol(data_unique))[sapply(data_unique,is.numeric)]
num_col_index3=num_col_index3[-1]
for(i in num_col_index3){
  col_min=min(data_unique[,i],na.rm = T)
  col_max=max(data_unique[,i],na.rm=T)
  data_unique[,i]=(data_unique[,i]-col_min)/(col_max-col_min)
  print(paste(colnames(data_unique)[i],col_max))
}
```

```{r}
data_unique=data_unique%>%
  select(
    -fireplacecnt,
    -garagetotalsqft,
    -poolcnt,
    -threequarterbathnbr,
    -numberofstories
  )
```


```{r}
head(data_unique)
```

```{r}
sapply(data_unique,function(x) length(unique(x)))
```

```{r}
data_unique=data_unique%>%select(-assessmentyear)
```


## split data into internal train and test sets


Create dummy variables
```{r}
data_transform=model.matrix(logerror~.-transactiondate-1, data=data_unique)
data_transform=as.data.frame(data_transform)
data_transform$logerror=data_unique$logerror
```

```{r}
head(data_transform)
```

remove large log error first

```{r}
hist(data_transform$logerror,n=100)
```


```{r}
data_transform=data_transform%>%filter(logerror<0.12 & logerror>-0.12)
```

```{r}
train_index=createDataPartition(y = data_transform$logerror,p = 0.8)
data_train_internal=data_transform[train_index$Resample1,]
data_test_internal=data_transform[-train_index$Resample1,]
```

```{r}
train_y=data_train_internal$logerror
train_x=as.matrix(data_train_internal%>%select(-logerror))

test_y=data_test_internal$logerror
test_x=as.matrix(data_test_internal%>%select(-logerror))
```

```{r}
dim(train_x)
```

```{r}
hist(train_y,n=200)
```


# Experiment with Neural Network
```{r}
library(keras)
# install_keras()
```


```{r}
model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 100, activation = 'tanh', input_shape = c(2186)) %>% 
  # layer_dropout(rate = 0.5) %>%
  # layer_dense(units = 100, activation = 'tanh') %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 1, activation = 'tanh')%>%
  compile(
  optimizer = optimizer_rmsprop(lr = 0.02),
  loss = 'mean_absolute_error'
)
```

```{r}
model %>% fit(train_x, train_y, epochs=10)

```

```{r}
summary(model)
```

predict test data
```{r}
predicted_test=predict(model,test_x)

plot(predicted_test,test_y)
```

